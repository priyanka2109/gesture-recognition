{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport os\n#imread to read images from data generator\nfrom imageio import imread\nfrom skimage.transform import resize\nfrom matplotlib.pyplot import imread\nimport datetime\nimport os\n#from scipy.misc.pilutil import imread","metadata":{"execution":{"iopub.status.busy":"2023-11-07T05:03:19.414316Z","iopub.execute_input":"2023-11-07T05:03:19.414676Z","iopub.status.idle":"2023-11-07T05:03:20.072597Z","shell.execute_reply.started":"2023-11-07T05:03:19.414648Z","shell.execute_reply":"2023-11-07T05:03:20.071807Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"np.random.seed(30)\nimport random as rn\nrn.seed(30)\nfrom keras import backend as K\nimport tensorflow as tf\ntf.random.set_seed(30)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T05:03:25.453107Z","iopub.execute_input":"2023-11-07T05:03:25.453760Z","iopub.status.idle":"2023-11-07T05:03:32.991370Z","shell.execute_reply.started":"2023-11-07T05:03:25.453728Z","shell.execute_reply":"2023-11-07T05:03:32.990541Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_doc = np.random.permutation(open('/kaggle/input/gesture-recognition/Project_data/train.csv').readlines())\nval_doc = np.random.permutation(open('/kaggle/input/gesture-recognition/Project_data/val.csv').readlines())\nbatch_size = 16","metadata":{"execution":{"iopub.status.busy":"2023-11-07T05:03:56.381958Z","iopub.execute_input":"2023-11-07T05:03:56.382615Z","iopub.status.idle":"2023-11-07T05:03:56.401987Z","shell.execute_reply.started":"2023-11-07T05:03:56.382579Z","shell.execute_reply":"2023-11-07T05:03:56.401152Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Generator\nThis is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with some of the parts of the generator function such that you get high accuracy.","metadata":{}},{"cell_type":"code","source":"def generator(source_path, folder_list, batch_size):\n    print( 'Source path = ', source_path, '; batch size =', batch_size)\n    img_idx = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28]\n    while True:\n        t = np.random.permutation(folder_list)\n        num_batches = int(len(t)/batch_size)\n        for batch in range(num_batches):\n            batch_data = np.zeros((batch_size,15,120,120,3))\n            batch_labels = np.zeros((batch_size,5))\n            for folder in range(batch_size):\n                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])\n                for idx,item in enumerate(img_idx):\n                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                    if image.shape[1] == 160:\n                        image = image[:,20:140,:].astype(np.float32)\n                    else:\n                        image = resize(image,(120,120)).astype(np.float32)\n                    \n                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 104\n                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 117\n                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 123\n                    \n                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n            yield batch_data, batch_labels\n\n        if (len(t)%batch_size) != 0:\n            batch_data = np.zeros((len(t)%batch_size,15,120,120,3))\n            batch_labels = np.zeros((len(t)%batch_size,5))\n            for folder in range(len(t)%batch_size):\n                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n                for idx,item in enumerate(img_idx):\n                    image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n                    if image.shape[1] == 160:\n                        image = image[:,20:140,:].astype(np.float32)\n                    else:\n                        image = resize(image,(120,120)).astype(np.float32)\n\n                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 104\n                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 117\n                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 123\n\n                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n\n            yield batch_data, batch_labels","metadata":{"execution":{"iopub.status.busy":"2023-11-07T05:03:59.805107Z","iopub.execute_input":"2023-11-07T05:03:59.805450Z","iopub.status.idle":"2023-11-07T05:03:59.822817Z","shell.execute_reply.started":"2023-11-07T05:03:59.805423Z","shell.execute_reply":"2023-11-07T05:03:59.822144Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"curr_dt_time = datetime.datetime.now()\ntrain_path = '/kaggle/input/gesture-recognition/Project_data/train'\nval_path = '/kaggle/input/gesture-recognition/Project_data/val'\nnum_train_sequences = len(train_doc)\nprint('# training sequences =', num_train_sequences)\nnum_val_sequences = len(val_doc)\nprint('# validation sequences =', num_val_sequences)\nnum_epochs = 10\nprint ('# epochs =', num_epochs)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T05:04:01.114982Z","iopub.execute_input":"2023-11-07T05:04:01.115325Z","iopub.status.idle":"2023-11-07T05:04:01.121215Z","shell.execute_reply.started":"2023-11-07T05:04:01.115298Z","shell.execute_reply":"2023-11-07T05:04:01.120255Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"# training sequences = 663\n# validation sequences = 100\n# epochs = 10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model\nHere you make the model using different functionalities that Keras provides. You might want to use `TimeDistributed`, `GRU` and other RNN structures after doing transfer learning. Also remember that the last layer is the softmax. Remember that the network is designed in such a way that the model is able to fit in the memory of the webcam.","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential, Model\nfrom keras.layers import Dense, GRU, Dropout, Flatten, TimeDistributed\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras import optimizers\nfrom keras.applications.vgg16 import VGG16\n    \nbase_model = VGG16(include_top=False, weights='imagenet', input_shape=(120,120,3))\nx = base_model.output\nx = Flatten()(x)\n#x.add(Dropout(0.5))\nfeatures = Dense(64, activation='relu')(x)\nconv_model = Model(inputs=base_model.input, outputs=features)\n    \nfor layer in base_model.layers:\n    layer.trainable = False\n        \nmodel = Sequential()\nmodel.add(TimeDistributed(conv_model, input_shape=(15,120,120,3)))\nmodel.add(GRU(32, return_sequences=True))\nmodel.add(GRU(16))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(5, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2023-11-07T05:04:11.266100Z","iopub.execute_input":"2023-11-07T05:04:11.266675Z","iopub.status.idle":"2023-11-07T05:04:15.781228Z","shell.execute_reply.started":"2023-11-07T05:04:11.266644Z","shell.execute_reply":"2023-11-07T05:04:15.780237Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58889256/58889256 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\nmodel.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nprint (model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-11-07T05:04:15.783221Z","iopub.execute_input":"2023-11-07T05:04:15.783718Z","iopub.status.idle":"2023-11-07T05:04:15.825143Z","shell.execute_reply.started":"2023-11-07T05:04:15.783681Z","shell.execute_reply":"2023-11-07T05:04:15.824118Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n time_distributed (TimeDistr  (None, 15, 64)           15009664  \n ibuted)                                                         \n                                                                 \n gru (GRU)                   (None, 15, 32)            9408      \n                                                                 \n gru_1 (GRU)                 (None, 16)                2400      \n                                                                 \n dropout (Dropout)           (None, 16)                0         \n                                                                 \n dense_1 (Dense)             (None, 8)                 136       \n                                                                 \n dense_2 (Dense)             (None, 5)                 45        \n                                                                 \n=================================================================\nTotal params: 15,021,653\nTrainable params: 306,965\nNon-trainable params: 14,714,688\n_________________________________________________________________\nNone\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super().__init__(name, **kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_generator = generator(train_path, train_doc, batch_size)\nval_generator = generator(val_path, val_doc, batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T05:04:18.346846Z","iopub.execute_input":"2023-11-07T05:04:18.347589Z","iopub.status.idle":"2023-11-07T05:04:18.351557Z","shell.execute_reply.started":"2023-11-07T05:04:18.347557Z","shell.execute_reply":"2023-11-07T05:04:18.350675Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model_name = 'model_init_conv_lstm' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n    \nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n        \nfilepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\nLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.0001, cooldown=0, min_lr=0.00001)\ncallbacks_list = [checkpoint, LR]","metadata":{"execution":{"iopub.status.busy":"2023-11-07T05:06:21.322499Z","iopub.execute_input":"2023-11-07T05:06:21.322857Z","iopub.status.idle":"2023-11-07T05:06:21.329857Z","shell.execute_reply.started":"2023-11-07T05:06:21.322829Z","shell.execute_reply":"2023-11-07T05:06:21.328937Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"if (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1","metadata":{"execution":{"iopub.status.busy":"2023-11-07T05:06:23.225111Z","iopub.execute_input":"2023-11-07T05:06:23.225805Z","iopub.status.idle":"2023-11-07T05:06:23.230960Z","shell.execute_reply.started":"2023-11-07T05:06:23.225774Z","shell.execute_reply":"2023-11-07T05:06:23.230023Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch.","metadata":{}},{"cell_type":"code","source":"model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n                    callbacks=callbacks_list, validation_data=val_generator, \n                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T05:06:26.807692Z","iopub.execute_input":"2023-11-07T05:06:26.808324Z","iopub.status.idle":"2023-11-07T05:18:28.411865Z","shell.execute_reply.started":"2023-11-07T05:06:26.808286Z","shell.execute_reply":"2023-11-07T05:18:28.410960Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Source path =  /kaggle/input/gesture-recognition/Project_data/train ; batch size = 16\nEpoch 1/10\n42/42 [==============================] - ETA: 0s - loss: 1.6606 - categorical_accuracy: 0.2097Source path =  /kaggle/input/gesture-recognition/Project_data/val ; batch size = 16\n\nEpoch 1: saving model to model_init_conv_lstm_2023-11-0705_04_01.117226/model-00001-1.66065-0.20965-1.61044-0.21000.h5\n42/42 [==============================] - 114s 2s/step - loss: 1.6606 - categorical_accuracy: 0.2097 - val_loss: 1.6104 - val_categorical_accuracy: 0.2100 - lr: 0.0010\nEpoch 2/10\n42/42 [==============================] - ETA: 0s - loss: 1.6277 - categorical_accuracy: 0.2051\nEpoch 2: saving model to model_init_conv_lstm_2023-11-0705_04_01.117226/model-00002-1.62771-0.20513-1.61240-0.13000.h5\n42/42 [==============================] - 67s 2s/step - loss: 1.6277 - categorical_accuracy: 0.2051 - val_loss: 1.6124 - val_categorical_accuracy: 0.1300 - lr: 0.0010\nEpoch 3/10\n42/42 [==============================] - ETA: 0s - loss: 1.6236 - categorical_accuracy: 0.1976\nEpoch 3: saving model to model_init_conv_lstm_2023-11-0705_04_01.117226/model-00003-1.62361-0.19759-1.60951-0.20000.h5\n42/42 [==============================] - 67s 2s/step - loss: 1.6236 - categorical_accuracy: 0.1976 - val_loss: 1.6095 - val_categorical_accuracy: 0.2000 - lr: 0.0010\nEpoch 4/10\n42/42 [==============================] - ETA: 0s - loss: 1.6189 - categorical_accuracy: 0.2142\nEpoch 4: saving model to model_init_conv_lstm_2023-11-0705_04_01.117226/model-00004-1.61888-0.21418-1.60919-0.21000.h5\n42/42 [==============================] - 66s 2s/step - loss: 1.6189 - categorical_accuracy: 0.2142 - val_loss: 1.6092 - val_categorical_accuracy: 0.2100 - lr: 0.0010\nEpoch 5/10\n42/42 [==============================] - ETA: 0s - loss: 1.6183 - categorical_accuracy: 0.2036\nEpoch 5: saving model to model_init_conv_lstm_2023-11-0705_04_01.117226/model-00005-1.61830-0.20362-1.60850-0.21000.h5\n42/42 [==============================] - 66s 2s/step - loss: 1.6183 - categorical_accuracy: 0.2036 - val_loss: 1.6085 - val_categorical_accuracy: 0.2100 - lr: 0.0010\nEpoch 6/10\n42/42 [==============================] - ETA: 0s - loss: 1.6141 - categorical_accuracy: 0.1961\nEpoch 6: saving model to model_init_conv_lstm_2023-11-0705_04_01.117226/model-00006-1.61409-0.19608-1.60916-0.22000.h5\n42/42 [==============================] - 67s 2s/step - loss: 1.6141 - categorical_accuracy: 0.1961 - val_loss: 1.6092 - val_categorical_accuracy: 0.2200 - lr: 0.0010\nEpoch 7/10\n42/42 [==============================] - ETA: 0s - loss: 1.6172 - categorical_accuracy: 0.2006\nEpoch 7: saving model to model_init_conv_lstm_2023-11-0705_04_01.117226/model-00007-1.61718-0.20060-1.60914-0.19000.h5\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n42/42 [==============================] - 67s 2s/step - loss: 1.6172 - categorical_accuracy: 0.2006 - val_loss: 1.6091 - val_categorical_accuracy: 0.1900 - lr: 0.0010\nEpoch 8/10\n42/42 [==============================] - ETA: 0s - loss: 1.6187 - categorical_accuracy: 0.1885\nEpoch 8: saving model to model_init_conv_lstm_2023-11-0705_04_01.117226/model-00008-1.61869-0.18854-1.60901-0.21000.h5\n42/42 [==============================] - 68s 2s/step - loss: 1.6187 - categorical_accuracy: 0.1885 - val_loss: 1.6090 - val_categorical_accuracy: 0.2100 - lr: 5.0000e-04\nEpoch 9/10\n42/42 [==============================] - ETA: 0s - loss: 1.6141 - categorical_accuracy: 0.2066\nEpoch 9: saving model to model_init_conv_lstm_2023-11-0705_04_01.117226/model-00009-1.61405-0.20664-1.60873-0.20000.h5\n\nEpoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n42/42 [==============================] - 68s 2s/step - loss: 1.6141 - categorical_accuracy: 0.2066 - val_loss: 1.6087 - val_categorical_accuracy: 0.2000 - lr: 5.0000e-04\nEpoch 10/10\n42/42 [==============================] - ETA: 0s - loss: 1.6137 - categorical_accuracy: 0.2097\nEpoch 10: saving model to model_init_conv_lstm_2023-11-0705_04_01.117226/model-00010-1.61366-0.20965-1.60903-0.21000.h5\n42/42 [==============================] - 70s 2s/step - loss: 1.6137 - categorical_accuracy: 0.2097 - val_loss: 1.6090 - val_categorical_accuracy: 0.2100 - lr: 2.5000e-04\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f8630358bb0>"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}